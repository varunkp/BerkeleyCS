import numpy as np
import scipy.io as sio
import gradient as gr
import prepare_data as pre
import sigmoid
import random
import math

train_small = sio.loadmat('train_small.mat')
train = sio.loadmat('train.mat')
test = sio.loadmat('test.mat')

train_small_data = train_small['train']
train_data = train['train']
test_data = test['test']

(features, labels) = pre.prepare_train_data(train_small_data)
def classify(x,W,b):
	W_bias = np.vstack((W,b))
	ones = np.ones((x.shape[0],1))
	x_bias = np.hstack((x, ones))

	y = np.divide(1.0,1.0 + np.exp(np.negative(np.transpose(np.dot(x_bias,W_bias)))))
       	return y

def SGD (y,x, eta_0, num_epochs):
	print x.shape
	W_old = np.random.random((784,10))
	bias_old = np.zeros((1, 10))
	batch_size = 200	
	indices = range(x.shape[0])
	t = [0] * 10
	t_mat = np.zeros((10, y.shape[0])) 

	x_batch = np.zeros((batch_size, x.shape[1]))
	y_batch = np.zeros((batch_size,))
	for j in xrange(t_mat.shape[1]):
		t_mat[y[j],j] = 1
	step = 0
	for i in xrange(num_epochs):
		random.shuffle (indices)
		print "Epoch " + str(i)
		for batch in xrange(x.shape[0]/ batch_size):
			print "Batch " + str(batch)
			step +=1
			eta = eta_0 * (1.0/math.sqrt(step))
			W_grad_sum = np.zeros((784,10))
			bias_grad_sum = np.zeros((1,10))
			chunk = indices[batch_size * batch: batch_size * (batch + 1)]
			x_batch = x[chunk, :] 
			y_batch =  y[chunk] 
			for i in xrange(x_batch.shape[0]): 
				t[y_batch[i]] = 1
				(W_grad, bias_grad) = gr.mean_squared_gradient(t,W_old,x_batch[i,:],bias_old) 
				W_grad_sum += W_grad
				bias_grad_sum += bias_grad
				t[y_batch[i]] = 0
			W_new = W_old - np.multiply(eta, W_grad_sum)
			W_old = W_new
			bias_new = bias_old - np.multiply(eta, bias_grad_sum)
			bias_old = bias_new
		#y is a d by n matrix

	

		y_class = classify(x, W_new, bias_new)
		J = 0.5 * (t_mat - y_class) ** 2
		error = np.sum(J)
		print error
		print y_class
	return (W_new, bias_new)

SGD(labels[3], features[3], .01, 50)
